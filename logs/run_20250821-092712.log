2025-08-21 09:27:12      INFO  Logging to logs/run_20250821-092712.log
2025-08-21 09:27:13      INFO  Dataset scan: meta=10000, kept=10, missing_images=9732, limit=10, seed=71, images_dir=data/MMFakeBench_test-001/MMFakeBench_test
2025-08-21 09:27:13      INFO  Dataset paths: DATA_JSON=data/MMFakeBench_test.json  IMAGES_DIR=data/MMFakeBench_test-001/MMFakeBench_test  LIMIT=10  SEED=71
2025-08-21 09:27:13      INFO  Dataset scan: meta=10000, kept=10, missing_images=9732, limit=10, seed=71, images_dir=data/MMFakeBench_test-001/MMFakeBench_test
2025-08-21 09:27:13      INFO  Pipeline start â€“ 10 samples, model=gemini-2.5-flash, chains=3, q_per_chain=3
2025-08-21 09:29:23      INFO  Branch 0 has no GOOD Q-A pairs.
2025-08-21 09:29:23      INFO  Branch 1 has no GOOD Q-A pairs.
2025-08-21 09:29:23      INFO  Branch 2 has no GOOD Q-A pairs.
2025-08-21 09:29:23     ERROR  Failed to process sample 0: 'FinalClassifier' object has no attribute '_has_time_aligned_text_refutation'
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/main_async.py", line 247, in main
    record = await process_sample(idx, sample)
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/main_async.py", line 164, in process_sample
    ).run(mr)
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/final_classifier.py", line 269, in run
    if not self._has_time_aligned_text_refutation():
AttributeError: 'FinalClassifier' object has no attribute '_has_time_aligned_text_refutation'
2025-08-21 09:29:38      INFO  ==== Pipeline metrics ====
qa_gate_min_answer_len=9  webqa_variant_added=2  webqa_backoff_no_gain=9  sample_failure=1
2025-08-21 09:29:38     ERROR  Task exception was never retrieved
future: <Task finished name='Task-1' coro=<main() done, defined at /teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/main_async.py:207> exception=KeyboardInterrupt()>
Traceback (most recent call last):
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/__main__.py", line 41, in <module>
    asyncio.run(_pipeline_main(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/runners.py", line 47, in run
    _cancel_all_tasks(loop)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/runners.py", line 63, in _cancel_all_tasks
    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/main_async.py", line 247, in main
    record = await process_sample(idx, sample)
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/main_async.py", line 154, in process_sample
    branches = await asyncio.gather(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py", line 636, in run_until_complete
    self.run_forever()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
    self._run_once()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/base_events.py", line 1909, in _run_once
    handle._run()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/asyncio/events.py", line 80, in _run
    self._context.run(self._callback, *self._args)
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/main_async.py", line 117, in generate_branch
    question, ok_q = q_tool.run(mr.get())
  File "/teamspace/studios/this_studio/Agentic-Workflow-LLM-Misinformation/scratch/qa_gen.py", line 297, in run
    resp = _get_chain(prompt_template, llm).invoke(variables)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/runnables/base.py", line 3046, in invoke
    input_ = context.run(step.invoke, input_, config)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 395, in invoke
    self.generate_prompt(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 980, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 799, in generate
    self._generate_with_cache(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py", line 1045, in _generate_with_cache
    result = self._generate(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 961, in _generate
    response: GenerateContentResponse = _chat_with_retry(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 196, in _chat_with_retry
    return _chat_with_retry(**kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tenacity/__init__.py", line 338, in wrapped_f
    return copy(f, *args, **kw)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tenacity/__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tenacity/__init__.py", line 378, in iter
    result = action(retry_state)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tenacity/__init__.py", line 400, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/tenacity/__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_google_genai/chat_models.py", line 178, in _chat_with_retry
    return generation_method(**kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py", line 835, in generate_content
    response = rpc(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py", line 131, in __call__
    return wrapped_func(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 294, in retry_wrapped_func
    return retry_target(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py", line 147, in retry_target
    result = target()
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/api_core/timeout.py", line 130, in func_with_timeout
    return func(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/api_core/grpc_helpers.py", line 76, in error_remapped_callable
    return callable_(*args, **kwargs)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/grpc/_interceptor.py", line 277, in __call__
    response, ignored_call = self._with_call(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/grpc/_interceptor.py", line 329, in _with_call
    call = self._interceptor.intercept_unary_unary(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/grpc.py", line 79, in intercept_unary_unary
    response = continuation(client_call_details, request)
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/grpc/_interceptor.py", line 315, in continuation
    response, call = self._thunk(new_method).with_call(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/grpc/_channel.py", line 1189, in with_call
    state, call = self._blocking(
  File "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/grpc/_channel.py", line 1162, in _blocking
    event = call.next_event()
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 388, in grpc._cython.cygrpc.SegregatedCall.next_event
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 211, in grpc._cython.cygrpc._next_call_event
  File "src/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi", line 205, in grpc._cython.cygrpc._next_call_event
  File "src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi", line 97, in grpc._cython.cygrpc._latent_event
  File "src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi", line 80, in grpc._cython.cygrpc._internal_latent_event
  File "src/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi", line 61, in grpc._cython.cygrpc._next
KeyboardInterrupt
